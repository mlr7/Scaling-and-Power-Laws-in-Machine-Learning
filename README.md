# Scaling-and-Power-Laws-in-Machine-Learning
A collection of papers, datasets, code, and links tracking scaling studies and power law behavior in machine learning

(this page is a work in progress)

## Scaling in Transformer-based Neural Language Models

#### Kaplan 2020: Scaling Laws for Neural Language Models

* Performance depends strongly on scale, weakly on model shape

* Smooth power laws

* Universality of overfitting

* Universality of training

* Transfer improves with test performance

* Sample efficiency

* Convergence is ineffecient

* Optimal batch size

## The Bitter Lesson, Richard Sutton

* http://www.incompleteideas.net/IncIdeas/BitterLesson.html

* https://www.kdnuggets.com/2020/11/revisiting-sutton-bitter-lesson-ai.html
