# Scaling-and-Power-Laws-in-Machine-Learning
A collection of papers, datasets, code, and links tracking scaling studies and power law behavior in machine learning

(this page is a work in progress)

## Scaling in Transformer-based Neural Language Models

#### Kaplan 2020: Scaling Laws for Neural Language Models

Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361 (2020).

https://arxiv.org/abs/2001.08361

Summary: 

* Performance depends strongly on scale, weakly on model shape

* Smooth power laws

* Universality of overfitting

* Universality of training

* Transfer improves with test performance

* Sample efficiency

* Convergence is ineffecient

* Optimal batch size

## The Bitter Lesson, Richard Sutton

* http://www.incompleteideas.net/IncIdeas/BitterLesson.html

* https://www.kdnuggets.com/2020/11/revisiting-sutton-bitter-lesson-ai.html
